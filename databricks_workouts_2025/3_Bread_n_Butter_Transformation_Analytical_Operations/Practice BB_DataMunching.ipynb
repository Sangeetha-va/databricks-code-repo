{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f82a136-fa0e-4398-94b1-448e9086c1a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Practice \"Data Munching\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f45dfc2a-27e1-4c8c-b0f0-1c710f7305df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Passive Data Munching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb1a71ec-5f9f-42f8-81ab-d86042fb39fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Read the data, create a data frame and identify its pattern\n",
    "rawdf=spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BB_Practice data/custsmodified\").toDF(\"id\",\"firstname\",\"lastname\",\"age\",\"profession\")\n",
    "#rawdf.show(20,False)\n",
    "print(\"display using take function\")\n",
    "display(rawdf.take(20))\n",
    "print(\"display using sampling\")\n",
    "display(rawdf.sample(.1))\n",
    "\n",
    "#Finding data structure\n",
    "print(\"print schema\")\n",
    "rawdf.printSchema() # this shows every col as string which means id and age has non numeric\n",
    "print(\"identifying Schema\")\n",
    "print(rawdf.schema) # prints schema structure, we can copy them and define a struct.\n",
    "print(\"print columns\")\n",
    "print(rawdf.columns)\n",
    "print(\"print data types\")\n",
    "print(rawdf.dtypes)\n",
    "\n",
    "#write datatype identification programatically\n",
    "for i in rawdf.dtypes:\n",
    "    if i[1]==\"string\":\n",
    "        print(i[0])\n",
    "\n",
    "#Finding total row count and duplicate row \n",
    "print(\"actual count of data\", rawdf.count()) #10005\n",
    "print(\"de-duplication of all columns\",rawdf.distinct().count()) #10004\n",
    "print(\"de-duplication of records using dropduplicate\",rawdf.dropDuplicates().count())\n",
    "print(\"de-duplicate of specified column\",rawdf.dropDuplicates(['id']).count())\n",
    "print(rawdf.describe())\n",
    "print(rawdf.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "626e365b-cb20-4fd4-80ec-6eede414dbaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Active Data munching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29176e80-4c40-44ec-bdb3-af533dcb90a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Wd-36 ETA Pipeline BB Practice\").getOrCreate()\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ShortType, DataType, DateType, time\n",
    "#Merge schema & Schema Evolution - 2 different files on same day with different schema, hence create 2 different schema and use unionByName to merge the schema, which ever is missing column, Null is filled in there.\n",
    "#Union is used when there is same datatype &  present on both files.\n",
    "struct1=StructType([StructField('id', StringType(), True), StructField('firstname', StringType(), True), StructField('lastname', StringType(), True), StructField('age', StringType(), True), StructField('profession', StringType(), True),StructField('corruptedrows', StringType(), True)])\n",
    "rawdf1=spark.read.schema(struct1).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/\",pathGlobFilter=\"custsmodified_N*\",recursiveFileLookup=True)\n",
    "display(rawdf1)\n",
    "struct2=StructType([StructField('id', StringType(), True), StructField('firstname', StringType(), True), StructField('age', StringType(), True), StructField('profession', StringType(), True),StructField('city', StringType(), True)])\n",
    "rawdf2=spark.read.schema(struct2).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/\",pathGlobFilter=\"custsmodified_T*\",recursiveFileLookup=True)\n",
    "display(rawdf2)\n",
    "rawdf_merged = rawdf1.unionByName(rawdf2,allowMissingColumns=True)\n",
    "display(rawdf_merged)\n",
    "\n",
    "#rawdf_merged = rawdf1.union(rawdf2)\n",
    "#display(rawdf_merged)#error - [NUM_COLUMNS_MISMATCH] UNION can only be performed on inputs with the same number of columns, but the first input has 5 columns and the second input has 6 columns. SQLSTATE: 42826\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "907dfee7-47a9-4a03-85b2-ad8567e45261",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Validation\n",
    "######-count: gives entire data count\n",
    "######-len(cleandf(collect())): gives only clean record count removing malformed\n",
    "######-display: displays in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8c7e6c9-dc86-4d9c-9c5c-a547746c9067",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Mode function- permissive/dropMalformed/\n",
    "cleandf = spark.read.schema(struct1).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BB_Practice data/custsmodified\",mode=\"permissive\")\n",
    "print(\"count displays entire data count\",cleandf.count()) #shows all 10005\n",
    "display(cleandf) \n",
    "#or \n",
    "cleandf = spark.read.schema(struct1).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BB_Practice data/custsmodified\",mode=\"dropMalformed\")\n",
    "print(\"count displays data count after removing malformed data\",len(cleandf.collect())) # this is costly since it reads all row length and gives the counts, shows 10002 removing 3 malformed records\n",
    "display(cleandf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12d02842-dc28-45d5-ad56-176b7583715d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Rejection strategy: remove all corrupted records/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67efc1f1-d878-48d9-9ee8-726debc605a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#in permissive mode separate corrupted datasets from good ones\n",
    "cleandf = spark.read.schema(struct1).csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BB_Practice data/custsmodified\",mode=\"permissive\",\n",
    "columnNameOfCorruptRecord=\"corruptedrows\")\n",
    "cleandf.printSchema()\n",
    "rejecteddf = cleandf.where(\"corruptedrows is not null\")\n",
    "display(rejecteddf)\n",
    "rejecteddf.write.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/custrejected\",mode=\"overwrite\",header=True)\n",
    "retaineddf = cleandf.where(\"corruptedrows is null\")\n",
    "display(retaineddf)\n",
    "print(\"show the entire data count\",len(cleandf.collect()))\n",
    "print(\"shows all rejected rows\",len(rejecteddf.collect()))\n",
    "print(\"shows all retained count after cleaning\",len(retaineddf.collect()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23f266b3-fd37-41fd-9932-aa5d8b5d018c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######cleaning - making data clean \n",
    "na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b69daaf-dc21-4d75-b9e7-8e3e08a0ac12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleanseddf = rawdf.na.drop(how=\"any\") # all dataset with null col are dropped.\n",
    "print(\"cleanseddf with all null col dropped\",len(cleanseddf.collect())) #9913\n",
    "display(rawdf.where(\"age is null\"))\n",
    "print(\"cleanseddf with any col. null\")\n",
    "display(cleanseddf.where(\"id is null\")) #returns no records which means all null datasets are dropped as expected\n",
    "# using subset to drop datasets based on specific columns while other columns with null are retained.\n",
    "print(\"cleansed data with any id col having null are dropped while other fields with null are retained\")\n",
    "cleanseddf = rawdf.na.drop(how=\"any\",subset=[\"id\"])\n",
    "display(cleanseddf.where(\"profession is null\"))\n",
    "print(\"count after eliminating all datasets having null in id and lastname\",len(cleanseddf.collect())) #9998 eliminates only null datasets\n",
    "cleanseddf = rawdf.na.drop(how=\"any\",subset=[\"firstname\",\"lastname\"])\n",
    "print(\"count after eliminating all datasets having null in firstname and lastname\",len(cleanseddf.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "827bdce0-ce3e-4bab-a5cc-f41add46b33f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Scrubbing\n",
    "na.fill() & na.replace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "567d64fa-d55c-423c-9c8a-e1ed7b3fafdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scrubbeddf = cleanseddf.na.fill(\"not provided\",subset=[\"age\",\"lastname\",\"profession\"])\n",
    "display(scrubbeddf.where (\"age is null\"))\n",
    "scrubbeddf2 = scrubbeddf.na.replace(\"not provided\",\"NA\",subset=[\"age\",\"lastname\",\"profession\"])\n",
    "display(scrubbeddf2)\n",
    "replace_scrubbed_data = {\"ten\":\"4000005\"}\n",
    "scrubbeddf3 = scrubbeddf2.na.replace(replace_scrubbed_data,subset=[\"id\"])\n",
    "display(scrubbeddf3)\n",
    "#display(scrubbeddf3.where(\"age is NA\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b43319ea-e950-47f6-a818-3b439e5c8a6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Deduplicate\n",
    "data.distinct() or data.dropduplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "311d1f0a-3ef0-4dc7-955b-09e914244ef6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(scrubbeddf2.where(\"id in ('4000001')\")) # 2 records displayed\n",
    "dedupdf = scrubbeddf2.distinct()\n",
    "display(dedupdf.where(\"id in ('4000001')\")) # 1 is dropped and distinct record shown\n",
    "dedupdf1 = dedupdf.coalesce(1).dropDuplicates(subset=[\"id\"])\n",
    "display(dedupdf1.where(\"id in ('4000003')\"))\n",
    "#display(dedupdf.coalesce(1).where(\"id in ('4000003')\").orderBy([\"id\",\"age\"],Descending=[False,False]))#need to check same value is returned with asc & desc\n",
    "dedupdf2 = (scrubbeddf2.coalesce(1).orderBy([\"id\",\"age\"],Ascending=[True,True]).dropDuplicates(subset=[\"id\"]))\n",
    "display(dedupdf2.where(\"id in ('4000003')\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "959485f1-0a86-4103-b1ca-57280ba63fe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Standardization\n",
    "withColumn/s(), withColumn/sRenamed(), regexp_replace(), select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59bf69cb-0b3b-4cb9-a4d4-9e8b17c91839",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Adding new column\n",
    "from pyspark.sql.functions import lit, initcap, col, upper, rlike, regexp_replace, replace\n",
    "\n",
    "stddf = dedupdf2.withColumn(\"sourcesystem\", lit(\"Retails\"))\n",
    "display(stddf.limit(10))\n",
    "\n",
    "#Data uniformity\n",
    "stddf1 = stddf.withColumn(\"profession\",initcap(col(\"profession\")))\n",
    "display(stddf1.limit(10))\n",
    "\n",
    "#Format standardization\n",
    "stddf1.where(\"id rlike '[a-zA-Z]'\").show()\n",
    "stddf1.where(\"age rlike '[^0-9]'\").show()\n",
    "\n",
    "std_replace_data = {\"ten\":\"4000005\"}\n",
    "stddf2 = scrubbeddf3.na.replace(std_replace_data,subset=[\"id\"])\n",
    "display(stddf2.where (\"id in ('4000005')\"))\n",
    "display(stddf2)\n",
    "stddf2 = stddf2.withColumn(\"age\",regexp_replace(col(\"age\"),\"-\",\"\"))\n",
    "display(stddf2) #7-7 is replaced as 77, this is expression replacement\n",
    "\n",
    "#Datatype conversion\n",
    "stddf2.printSchema()\n",
    "stddf3 = stddf2.withColumn(\"id\",col(\"id\").cast(\"long\"))\n",
    "stddf3.printSchema()\n",
    "stddf3 = stddf3.withColumn(\"age\",col(\"age\").cast(\"short\"))\n",
    "stddf3.printSchema()\n",
    "stddf3 = stddf3.withColumn(\"sourcesystem\", lit(\"Retails\"))\n",
    "\n",
    "#using withColumn/sRenamed() to rename the header column\n",
    "stddf4 = stddf3.withColumnsRenamed({\"id\":\"custid\",\"profession\":\"prof\"})\n",
    "display(stddf4)\n",
    "\n",
    "# to see columns in different order from given order use \"select\"\n",
    "stddf5 = stddf4.select('custid','firstname','lastname','prof','age','sourcesystem')\n",
    "\n",
    "# before egress check data quality & data integrity\n",
    "mungeddf = stddf5\n",
    "mungeddf.printSchema()\n",
    "print(\"row count\",len(mungeddf.collect()))\n",
    "print(mungeddf.schema)\n",
    "display(mungeddf.summary)\n",
    "\n",
    "stddf5.write.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/BB_Practice data/custsmodified_target\",mode=\"overwrite\",header=True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Practice BB_DataMunching",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
