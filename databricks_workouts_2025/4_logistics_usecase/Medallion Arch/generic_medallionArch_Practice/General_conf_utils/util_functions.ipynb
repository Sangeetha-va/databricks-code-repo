{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f62421e2-1057-4678-97d9-2089f0c6cc16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####This Notebook has UDF, generic framework - business functions & common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92ef8334-4964-4d92-aa0d-3460f0cc7024",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Running utility notebook to initialize all functions for further use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2d6afae-566c-4d97-ab05-7120127d4cfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### UDF created to convert string to number word2number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "588fd75d-4ed3-4cf4-8129-263ab833a0f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install word2number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8508c7a5-d89e-4981-97e4-9be29245401e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import IntegerType\n",
    "from word2number import w2n\n",
    "\n",
    "def word_to_num(value):\n",
    "    try:\n",
    "        return(int(value))\n",
    "    except:\n",
    "        try:\n",
    "            return w2n.word_to_num(value.lower())\n",
    "        except:\n",
    "            return None    \n",
    "word_to_num_udf = udf(word_to_num, IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72af8484-4792-4725-b157-06bcc5f287a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Business Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "433005a9-7cc9-443f-abb2-25cde138935d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F \n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "def standardize_staff(df):\n",
    "    return(df\n",
    "           .withColumn(\"shipment_id\",word_to_num_udf(F.col(\"shipment_id\")).cast(\"long\"))\n",
    "           .withColumn(\"age\",word_to_num_udf(F.col(\"age\")).cast(\"int\"))\n",
    "           .withColumn(\"hub_location\",F.initcap(\"hub_location\"))\n",
    "           .withColumn(\"role\",F.lower(\"role\"))\n",
    "           .withColumn(\"loaddt\",F.current_timestamp())\n",
    "           .withColumn(\"origin_hub_city\",F.initcap(\"hub_location\"))\n",
    "           .withColumn(\"fullname\",F.concat_ws(\" \",\"first_name\",\"last_name\"))\n",
    "           .drop(\"first_name\",\"last_name\")\n",
    "           .withColumnRenamed(\"fullname\",\"staff_full_name\")\n",
    "    )\n",
    "\n",
    "def scrub_geotag(df):\n",
    "    return(df\n",
    "        .withColumn(\"city_name\",F.initcap(\"city_name\"))\n",
    "        .withColumn(\"masked_hub_location\",F.initcap(\"country\"))\n",
    "    )\n",
    "        \n",
    "def standardize_shipments(df):\n",
    "    return (df\n",
    "        .withColumn(\"domain\", F.lit(\"Logistics\"))\n",
    "        .withColumn(\"ingestion_timestamp\", F.current_timestamp())\n",
    "        .withColumn(\"is_expedited\", F.lit(False).cast(\"boolean\"))\n",
    "        .withColumn(\"shipment_date\", F.to_date(\"shipment_date\", \"yy-MM-dd\"))\n",
    "        .withColumn(\"shipment_cost\", F.round(\"shipment_cost\", 2))\n",
    "        .withColumn(\"shipment_weight_kg\", F.col(\"shipment_weight_kg\").cast(\"double\"))\n",
    "    )\n",
    "\n",
    "def enrich_shipments(df):\n",
    "    return (df\n",
    "        .withColumn(\"route_segment\",\n",
    "            F.concat_ws(\"-\", \"source_city\", \"destination_city\"))\n",
    "        .withColumn(\"vehicle_identifier\",\n",
    "            F.concat_ws(\"_\", \"vehicle_type\", \"shipment_id\"))\n",
    "        .withColumn(\"shipment_year\", F.year(\"shipment_date\"))\n",
    "        .withColumn(\"shipment_month\", F.month(\"shipment_date\"))\n",
    "        .withColumn(\"is_weekend\",\n",
    "            F.dayofweek(\"shipment_date\").isin([1,7]))\n",
    "        .withColumn(\"is_expedited\",\n",
    "            F.col(\"shipment_status\").isin(\"IN_TRANSIT\", \"DELIVERED\"))\n",
    "        .withColumn(\"cost_per_kg\",\n",
    "            F.round(F.col(\"shipment_cost\") / F.col(\"shipment_weight_kg\"), 2))\n",
    "        .withColumn(\"tax_amount\",\n",
    "            F.round(F.col(\"shipment_cost\") * 0.18, 2))\n",
    "        .withColumn(\"days_since_shipment\",\n",
    "            F.datediff(F.current_date(), \"shipment_date\"))\n",
    "        .withColumn(\"is_high_value\",\n",
    "            F.col(\"shipment_cost\") > 50000)\n",
    "    )\n",
    "\n",
    "def split_col(df):\n",
    "    return(df\n",
    "           .withColumn(\"order_prefix\",F.substring(\"order_id\",1,3))\n",
    "           .withColumn(\"order_seq\",F.substring(\"order_id\",4,10))\n",
    "           .withColumn(\"shipment_year\",F.year(\"shipment_date\"))\n",
    "           .withColumn(\"shipment_month\",F.month(\"shipment_date\"))\n",
    "           .withColumn(\"shipment_day\",F.dayofmonth(\"shipment_date\"))\n",
    "           .withColumn(\"route_lane\",F.concat_ws(\"->\",\"source_city\",\"destination_city\"))\n",
    "           )\n",
    "    \n",
    "def masked_name(col_name):\n",
    "    return F.concat(\n",
    "        F.substring(col_name,1,2),\n",
    "        F.lit('*****'),\n",
    "        F.substring(col_name,-1,1)\n",
    "                    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95acf801-4a1a-4daf-ac8f-0200ee3e19a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Generic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caa68931-3bd2-433f-8324-97ed48e6f6e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "def get_spark_session(appname=\"logistic usescases practice with FWs\"):\n",
    "    try:\n",
    "        spark = SparkSession.getActiveSession()\n",
    "        if spark:\n",
    "            return spark\n",
    "    except:\n",
    "        pass    \n",
    "    return (SparkSession.builder.config(\"spark.sql.shuffle.partitions\",\"1\").appName(appname).getOrCreate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddeed1fa-8917-45b0-be49-c7f99afc8071",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read files functions\n",
    "\n",
    "def read_csv_df(spark,path,header=True,infer_schema=True,sep=\",\"):\n",
    "    return_df =spark.read.option(\"header\", header).\\\n",
    "                      option(\"infer_schema\",infer_schema).\\\n",
    "                      option(\"sep\",sep).\\\n",
    "                      csv(path)\n",
    "    return return_df\n",
    "\n",
    "def read_json_df(spark, path,mline=True):\n",
    "    return spark.read.json(path,multiLine=mline,mode=\"PERMISSIVE\")\n",
    "\n",
    "def read_delta_df(spark,path):\n",
    "    return spark.read.format(\"delta\").load(path)\n",
    "\n",
    "def read_file(spark,filetype,path,header=True,infer_schema=True,mline=True):\n",
    "    if filetype==\"csv\":\n",
    "        return spark.read.csv(path,header=header,inferSchema=infer_schema)#read_csv_df(spark,path)\n",
    "    elif filetype==\"json\":\n",
    "        return read_json_df(spark,path)\n",
    "    elif filetype==\"delta\":\n",
    "        return read_delta_df(spark,path)\n",
    "    elif filetype=='orc':\n",
    "        return spark.read.orc(path)\n",
    "    else:\n",
    "        raise Exception(\"File type not supported\")\n",
    "\n",
    "def read_table(spark,table_name):\n",
    "    return spark.table(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2db4c81e-f006-41ee-b2d3-9bf5244cf558",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#joindf\n",
    "def join_df(df1,df2,how='inner', on='shipment_id'):\n",
    "    return df1.join(df2, on=on, how=how)\n",
    "\n",
    "def union_df(df1,df2):\n",
    "    return df1.union(df2)\n",
    "\n",
    "def unionDfSql(spark, view1, view2):\n",
    "    return_df = spark.sql(f\"select * from {view1} union select * from {view2}\")\n",
    "    return return_df\n",
    "\n",
    "def mergeDf(df1,df2,allowmissingcol=True):\n",
    "    return df1.unionByName(df2, allowMissingColumns=allowmissingcol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29ffaab5-9603-4152-92f4-70481ae86fdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "def add_literal_columns(df, columns, default_value=None):\n",
    "    for col_name in columns:\n",
    "        df = df.withColumn(col_name, lit(default_value))\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "614abb11-6ca1-44d2-8535-b27c0a7ea362",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#write operation\n",
    "def write_file(df, path, mode=\"overwrite\", format=\"delta\"):\n",
    "    return df.write.mode(mode).format(format).save(path)\n",
    "\n",
    "def write_table(df, tablename, mode=\"overwrite\"):\n",
    "    df.write.mode(mode).format(\"delta\").saveAsTable(tablename)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b54dd5b9-e815-40c4-a377-f208c58f981f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def return_tempview_distinct_df(spark,tempview):\n",
    "    return spark.sql(f\"select distinct * from {tempview}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "util_functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
