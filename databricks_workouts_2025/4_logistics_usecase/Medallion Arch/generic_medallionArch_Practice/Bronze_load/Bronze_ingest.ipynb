{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19696526-d0b6-4824-b996-af344c6d456c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Ingestion from source datalake to bronze layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd74bd63-3258-4368-ae6d-aba71cb4ae26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"catalog\",\"\")\n",
    "CATALOG = dbutils.widgets.get(\"catalog\").strip()\n",
    "dbutils.widgets.text(\"schema\",\"\")\n",
    "SCHEMA = dbutils.widgets.get(\"schema\").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db790bcc-8ad4-408e-8dc2-3cc5fc0f7ea2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Setting generic configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "038db1c2-606b-4a48-9234-43d29bf4dd1b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setting generic configuration"
    }
   },
   "outputs": [],
   "source": [
    "import json # since json is used in both child (json.dumps()) and parent (json.load()) notrbooks it is required to import json in both notebooks.\n",
    "\n",
    "# when we run the child notebook, thru exit() we will receive the string values to make a dict. we have to parse by calling it as json.loads().\n",
    "\n",
    "config_nb_out = dbutils.notebook.run(\n",
    "    \"/Workspace/Users/sangeetha.va@outlook.com/databricks-code-repo/databricks_workouts_2025/4_logistics_usecase/Medallion Arch/generic_medallionArch_Practice/General_conf_utils/configs_path\",120,{\"catalog\":CATALOG,\"schema\":SCHEMA})\n",
    "\n",
    "#storing the output from child notebook to a parent notebook variable.\n",
    "config_dict = json.loads(config_nb_out)\n",
    "print(config_dict)\n",
    "\n",
    "#storing src & bronze file location to variable in parent notebook\n",
    "SRC = config_dict[\"SRC\"]\n",
    "BRONZE = config_dict[\"BRONZE\"]\n",
    "\n",
    "print(\"source location\", SRC)\n",
    "print(\"bronze location\",BRONZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14258b91-8aee-4350-845d-6b074062b0df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "515353c0-f48c-46ae-b5e8-fd992c9b934f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run \"/Workspace/Users/sangeetha.va@outlook.com/databricks-code-repo/databricks_workouts_2025/4_logistics_usecase/Medallion Arch/generic_medallionArch_Practice/General_conf_utils/util_functions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6ce87e7-2618-41d7-8d79-80bd5790205e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####All Generic Functions can be defined here like spark session initiation/read opn/write opn etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e11a7e24-8e2b-40f8-a25d-8fc36aef6aa5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Untitled"
    }
   },
   "outputs": [],
   "source": [
    "spark=get_spark_session(\"logistics medallion arch-FW practice\")\n",
    "\n",
    "#from pyspark.sql.sessions import SparkSession\n",
    "# in general platform would have spark session instatiated automatically, yet if no active session then config and create one\n",
    "'''def get_spark_session(app_name=\"Medallion-Arch Practice\"):\n",
    "    try:\n",
    "        spark = SparkSession.getActiveSession() #calling active session if any & rtn\n",
    "        if spark:\n",
    "            return spark\n",
    "    except:\n",
    "        pass \n",
    "    return (SparkSession.builder.config (\"spark.sql.shuffle.partitions\", \"1\").appName(app_name).getOrCreate())'''\n",
    "    # here builder is entry point for config the current spark session. further spark sql config is done for 1 partition since its small vol. of data, by default it would use 200. finally get or create a session.\n",
    "\n",
    "'''We lost all the below features if using inline functions...\n",
    "    Centralized and controllable\n",
    "    Production Ready\n",
    "    Reusability\n",
    "    Seperation of Concern\n",
    "    Modularized\n",
    "    Simple to write/Reasonable to understand\n",
    "    Optimization\n",
    "    Governed\n",
    "    Secured\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b366ac77-3c31-4bd2-ba1f-29149d939db9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read all files and tables\n",
    "\n",
    "staff1 = read_file(spark,'csv',f\"{SRC}/logistics_source1.txt\",True,False)# using function in if/elif section\n",
    "staff2 = read_csv_df(spark,f\"{SRC}/logistics_source2.txt\",True,False)# using function defined seperately in util. notebook\n",
    "print(staff1)\n",
    "print(staff2)\n",
    "\n",
    "staff_bronze = mergeDf(staff1,staff2,True) # since src1 & 2 has different structure we are using merge opn.\n",
    "\n",
    "#geotag file read\n",
    "geo_tagging=read_csv_df(spark,f\"{SRC}/Master_City_List.csv\",True,False)\n",
    "\n",
    "#Shipment data read\n",
    "shipments_bronze = read_json_df(spark,f\"{SRC}/logistics_shipment_detail_3000.json\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "597f1b07-ffd9-45e5-b7f6-f706d1108bfa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Write bronze files with path check"
    }
   },
   "outputs": [],
   "source": [
    "print(\"source location\", SRC)\n",
    "print(\"bronze location\", BRONZE)\n",
    "\n",
    "#write_file(staff1, f\"{BRONZE}/staff\", mode=\"overwrite\", format=\"delta\") \n",
    "'''- to debug issue, tried to write staff1 to bronze. later when issue was resolved, I wanted to write merged stage_bronze to bronze target which has differnt schema.Hence to overwrite schema without deleting the existing file I can use option(\"overwriteSchema\",\"True\"). this works only when mode=\"overwrite\".'''\n",
    "\n",
    "write_file(staff_bronze, f\"{BRONZE}/staff\", mode=\"overwrite\", format=\"delta\")\n",
    "write_file(geo_tagging, f\"{BRONZE}/geotag\", mode=\"overwrite\", format=\"delta\")\n",
    "write_file(shipments_bronze, f\"{BRONZE}/shipments\", mode=\"overwrite\", format=\"delta\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze_ingest",
   "widgets": {
    "catalog": {
     "currentValue": "prodcatalogdef",
     "nuid": "b683c182-7e69-4bc7-9ce0-d31ce196e760",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema": {
     "currentValue": "prodschemadef",
     "nuid": "e4339ddd-48e4-49c5-85d2-dd856d6f5def",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
