{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4e9db29-71e6-4ee6-a139-a2e6adafec63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Enterprise Fleet Analytics Pipeline: Focuses on the business outcome (analytics) and the domain (fleet/logistics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60102b53-da15-4c74-a306-b675d15c78d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![logistics](https://raw.githubusercontent.com/mohamedirfankader/databricks-code-repo/main/4_logistics_usecase/logistics_project.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c798b032-39ac-4bf6-9992-78159146fb4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Download the data from the below gdrive and upload into the catalog\n",
    "https://drive.google.com/drive/folders/1J3AVJIPLP7CzT15yJIpSiWXshu1iLXKn?usp=drive_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d7bc63d-ea3c-45ee-a8e5-33af905b79da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--create schema if not exists logistics_catalog.landing_zone;\n",
    "--create volume if not exists logistics_catalog.landing_zone.landing_vol;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7d1e2d4-18b8-4cfa-b450-ac94f05ff640",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "spark1 = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a18f7c6d-3a67-49dc-bf5a-e7fe02152044",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_path = \"/Volumes/logistics_catalog/landing_zone/landing_vol/\"\n",
    "loaddata = dbutils.fs.mkdirs(base_path + \"Source_data/\")\n",
    "shipdata = dbutils.fs.mkdirs(base_path + \"shipment_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81ca9127-636a-4be2-ad66-6af734d83aa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##**1. Data Munging** -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73641445-2b65-4138-89f3-44e10449c278",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####1. Visibily/Manually opening the file and capture couple of data patterns (Manual Exploratory Data Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95769146-c3fc-481e-a1bf-90640696cfd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####shipment data:\n",
    "######1. date field in yy-mm-dd format\n",
    "######2. status field has special character in string- \"IN_TRANSIT\"\n",
    "######3. HAS 3K records\n",
    "\n",
    "#####source1 & 2:\n",
    "######1. comma seperated values with <100 records\n",
    "######2. Duplicate rows & id keys present (6000002)\n",
    "######3. Null records are present\n",
    "######4. missing few columns values in multiple records like firstname, lastname, vehicle type, every column except id etc.\n",
    "######5. integer values are in string like 10 as ten(age and id columns) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "507bfd1d-8d3d-417b-8da8-074150a9eec3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####2. Programatically try to find couple of data patterns applying below EDA (File: logistics_source1)\n",
    "1. Apply inferSchema and toDF to create a DF and analyse the actual data.\n",
    "2. Analyse the schema, datatypes, columns etc.,\n",
    "3. Analyse the duplicate records count and summary of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2533368f-4c5c-426c-aace-9da04f0dd2e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1\n",
    "rawsrc1df = spark.read.csv(\"/Volumes/logistics_catalog/landing_zone/landing_vol/Source_data/logistics_source1\",header=True,inferSchema=True).toDF(\"shipment_id\",\"first_name\",\"last_name\",\"age\",\"role\")\n",
    "display(rawsrc1df)\n",
    "# how header = false/true or not mentioning header when using toDf differ from each other.\n",
    "\n",
    "#2\n",
    "rawsrc1df.printSchema()\n",
    "print(\"schema\")\n",
    "display(rawsrc1df.schema)\n",
    "display(rawsrc1df.dtypes)\n",
    "print(\"columns\")\n",
    "display(rawsrc1df.columns)\n",
    "print(\"total record count\", rawsrc1df.count())\n",
    "\n",
    "#3 \n",
    "print(\"deduplicate using distinct func\",rawsrc1df.distinct().count())\n",
    "print(\"deduplicate using dropduplicate func\",rawsrc1df.dropDuplicates().count())\n",
    "print(\"deduplicate using dropduplicate with id col filter\",rawsrc1df.dropDuplicates([\"shipment_id\"]).count())\n",
    "\n",
    "print(\"summary\")\n",
    "display(rawsrc1df.summary())\n",
    "print(\"Describe\")\n",
    "print(rawsrc1df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de27b31f-de9a-457b-be06-00ada4960419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###a. Passive Data Munging -  (File: logistics_source1  and logistics_source2)\n",
    "Without modifying the data, identify:<br>\n",
    "Shipment IDs that appear in both master_v1 and master_v2<br>\n",
    "Records where:<br>\n",
    "1. shipment_id is non-numeric\n",
    "2. age is not an integer<br>\n",
    "\n",
    "Count rows having:\n",
    "3. fewer columns than expected\n",
    "4. more columns than expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ae841bd6-2132-446f-8b63-de6325f48666",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rawsrc2df = spark.read.csv(\"/Volumes/logistics_catalog/landing_zone/landing_vol/Source_data/logistics_source2\",header=True)\n",
    "\n",
    "#Shipment IDs that appear in both master_v1 and master_v2, Records where:\n",
    "s12joindf = rawsrc1df.alias(\"l\").join(rawsrc2df.alias(\"r\"),how=\"inner\",on=([\"first_name\",\"last_name\"]))\n",
    "display(s12joindf)\n",
    "#1.shipment_id is non-numeric\n",
    "src1idnndf1 = s12joindf.select(\"l.shipment_id\").where(\"shipment_id rlike '[^0-9]'\") \n",
    "display(src1idnndf1)\n",
    "src1idnndf2 = s12joindf.select(\"l.shipment_id\",\"l.age\").where(\"age rlike '[^0-9]'\")\n",
    "display(src1idnndf2)\n",
    "\n",
    "src2idnndf1 = s12joindf.select(\"r.shipment_id\").where(\"shipment_id rlike '[^0-9]'\") \n",
    "display(src2idnndf1)\n",
    "src2idnndf2 = s12joindf.select(\"r.shipment_id\",\"r.age\").where(\"age rlike '[^0-9]'\")\n",
    "display(src2idnndf2)\n",
    "\n",
    "#Count rows having:\n",
    "#:3. fewer columns than expected - below both doesnt show records that has null in name cols.\n",
    "src1missingcoldf1 = s12joindf.select(\"l.shipment_id\",\"l.first_name\",\"l.last_name\",\"l.age\",\"l.role\").where(\"l.first_name is null or l.last_name is null or l.age is null or l.role is null\")\n",
    "display(src1missingcoldf1)#source1 has 3 records with fewer cols than expected\n",
    "\n",
    "src2missingcoldf1 = s12joindf.select(\"r.shipment_id\",\"r.first_name\",\"r.last_name\",\"r.age\",\"r.role\",\"r.hub_location\",\"r.vehicle_type\").where(\"r.first_name is null or r.last_name is null or r.age is null or r.role is null or r.hub_location is null or r.vehicle_type is null\")\n",
    "display(src2missingcoldf1)#source2 has 13 records with fewer cols than expected\n",
    "\n",
    "#4. more columns than expected - ask and learn how to do it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13f97700-cae1-4ab6-97e8-502ffbd1a6d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create a Spark Session Object\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark1 = SparkSession.builder.appName(\"logistics usecases\").getOrCreate()\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ShortType, TimestampType, DateType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c81fc736-6e9f-4093-9f40-8e047989b602",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###**b. Active Data Munging** File: logistics_source1 and logistics_source2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7b773a5-b1db-4b1f-bf3e-93067d0483ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####1.Combining Data + Schema Merging (Structuring)\n",
    "1. Read both files without enforcing schema\n",
    "2. Align them into a single canonical schema: shipment_id,\n",
    "first_name,\n",
    "last_name,\n",
    "age,\n",
    "role,\n",
    "hub_location,\n",
    "vehicle_type,\n",
    "data_source\n",
    "3. Add data_source column with values as: system1, system2 in the respective dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "106a7a34-c5f0-4665-92e6-c8210c024271",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1.Read both files without enforcing schema\n",
    "src1df = spark.read.csv(\"/Volumes/logistics_catalog/landing_zone/landing_vol/Source_data/\",header=True,mode=\"PERMISSIVE\",pathGlobFilter=\"logistics_source1\",recursiveFileLookup=True)\n",
    "#display(src1df)\n",
    "#display(src1df.schema)\n",
    "src2df = spark.read.csv(\"/Volumes/logistics_catalog/landing_zone/landing_vol/Source_data/\",header=True,mode=\"PERMISSIVE\",pathGlobFilter=\"logistics_source2\",recursiveFileLookup=True)\n",
    "#display(src2df)\n",
    "#display(src2df.schema)\n",
    "#3.Add data_source column with values as: system1, system2 in the respective dataframes\n",
    "src1df = src1df.withColumn(\"data_source\",lit(\"system1\"))\n",
    "src2df = src2df.withColumn(\"data_source\",lit(\"system2\"))\n",
    "#2.Align them into a single canonical schema: shipment_id, first_name, last_name, age, role, hub_location, vehicle_type, data_source\n",
    "mergesrc12df = src1df.unionByName(src2df,allowMissingColumns=True)\n",
    "#display(mergesrc12df)\n",
    "#display(mergesrc12df.schema)\n",
    "#StructType([StructField('shipment_id', StringType(), True), StructField('first_name', StringType(), True), StructField('last_name', StringType(), True), StructField('age', StringType(), True), StructField('role', StringType(), True)])\n",
    "#StructType([StructField('shipment_id', StringType(), True), StructField('first_name', StringType(), True), StructField('last_name', StringType(), True), StructField('age', StringType(), True), StructField('role', StringType(), True), StructField('hub_location', StringType(), True), StructField('vehicle_type', StringType(), True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "628e4769-0e24-481b-8b5c-33204a91ed3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####2. Cleansing, Scrubbing: \n",
    "Cleansing (removal of unwanted datasets)<br>\n",
    "1. Mandatory Column Check - Drop any record where any of the following columns is NULL:shipment_id, role<br>\n",
    "2. Name Completeness Rule - Drop records where both of the following columns are NULL: first_name, last_name<br>\n",
    "3. Join Readiness Rule - Drop records where the join key is null: shipment_id<br>\n",
    "\n",
    "Scrubbing (convert raw to tidy)<br>\n",
    "4. Age Defaulting Rule - Fill NULL values in the age column with: -1<br>\n",
    "5. Vehicle Type Default Rule - Fill NULL values in the vehicle_type column with: UNKNOWN<br>\n",
    "6. Invalid Age Replacement - Replace the following values in age:\n",
    "\"ten\" to -1\n",
    "\"\" to -1<br>\n",
    "7. Vehicle Type Normalization - Replace inconsistent vehicle types: \n",
    "truck to LMV\n",
    "bike to TwoWheeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d2e028f-b34a-409a-b9aa-3874d26b531f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Cleansing (removal of unwanted datasets)\n",
    "#1.Mandatory Column Check - Drop any record where any of the following columns is NULL:shipment_id, role\n",
    "cleandedupdf = mergesrc12df.na.drop(how=\"any\",subset=[\"shipment_id\",\"role\"])\n",
    "#display(cleandedupdf.filter(\"shipment_id is null or role is null\")) \n",
    "#display(cleandedupdf)\n",
    "#2.Name Completeness Rule - Drop records where both of the following columns are NULL: first_name, last_name\n",
    "cleandedup2df = cleandedupdf.na.drop(how=\"any\",subset=[\"first_name\",\"last_name\"])\n",
    "#display(cleandedup2df.filter(\"first_name is null or last_name is null\"))\n",
    "#display(cleandedup2df)\n",
    "#3.Join Readiness Rule - Drop records where the join key is null: shipment_id\n",
    "#display(cleandedup2df.filter(\"shipment_id is null\"))\n",
    "cleansrc12df = cleandedup2df\n",
    "display(cleansrc12df)\n",
    "\n",
    "#Scrubbing (convert raw to tidy)\n",
    "#4. Age Defaulting Rule - Fill NULL values in the age column with: -1\n",
    "scrubsrc12df = cleansrc12df.na.fill('-1', subset=[\"age\"])\n",
    "#display(scubsrc12df.filter(\"age == -1\"))\n",
    "#display(scrubsrc12df)\n",
    "#5. Vehicle Type Default Rule - Fill NULL values in the vehicle_type column with: UNKNOWN\n",
    "scrubsrc12df = scrubsrc12df.na.fill('UNKNOWN', subset=[\"vehicle_type\"])\n",
    "#display(scrubsrc12df.filter(\"vehicle_type == 'UNKNOWN'\"))\n",
    "#display(scrubsrc12df)\n",
    "#6. Invalid Age Replacement - Replace the following values in age: \"ten\" to -1 \"\n",
    "scrubsrc12df = scrubsrc12df.na.replace('ten','-1', subset=[\"age\"])\n",
    "#display(scrubsrc12df.filter(\"age == -1\"))\n",
    "#display(scrubsrc12df)\n",
    "#7. Vehicle Type Normalization - Replace inconsistent vehicle types: truck to LMV bike to TwoWheeler\n",
    "#display(scrubsrc12df.where(\"vehicle_type in ('Truck','Bike')\"))\n",
    "scrubsrc12df = scrubsrc12df.na.replace({'Truck':'LMV','Bike':'TwoWheeler'}, subset=[\"vehicle_type\"])\n",
    "#display(scrubsrc12df.where(\"vehicle_type in ('LMV','TwoWheeler')\"))\n",
    "display(scrubsrc12df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b089e58-4b74-41e5-b050-bbfa8d249467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####3. Standardization, De-Duplication and Replacement / Deletion of Data to make it in a usable format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ccc60af7-9398-4de0-93ce-1f5bf9dd5c47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Creating shipments Details data Dataframe creation <br>\n",
    "1. Create a DF by Reading Data from logistics_shipment_detail.json\n",
    "2. As this data is a clean json data, it doesn't require any cleansing or scrubbing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a46df34d-1ffa-42d5-918e-755f7d1c56eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "shipmentsrcdf = spark.read.format(\"json\").load(\"/Volumes/logistics_catalog/landing_zone/landing_vol/shipment_data/logistics_shipment_detail_3000.json\",primitivesAsString=True,columnNameOfCorruptRecord=\"corruptrows\",multiline=True)\n",
    "print(\"json file read\")\n",
    "shipmentsrcdf.printSchema()\n",
    "display(shipmentsrcdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cd9b438-99be-431d-a81a-493c23b2b998",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Standardizations:<br>\n",
    "\n",
    "1. Add a column<br> \n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>: domain as 'Logistics',  current timestamp 'ingestion_timestamp' and 'False' as 'is_expedited'\n",
    "2. Column Uniformity: \n",
    "role - Convert to lowercase<br>\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2)<br>\n",
    "vehicle_type - Convert values to UPPERCASE<br>\n",
    "Source Files: DF of logistics_shipment_detail_3000.json\n",
    "hub_location - Convert values to initcap case<br>\n",
    "Source Files: DF of merged(logistics_source1 & logistics_source2)<br>\n",
    "3. Format Standardization:<br>\n",
    "Source Files: DF of logistics_shipment_detail_3000.json<br>\n",
    "Convert shipment_date to yyyy-MM-dd<br>\n",
    "Ensure shipment_cost has 2 decimal precision<br>\n",
    "4. Data Type Standardization<br>\n",
    "Standardizing column data types to fix schema drift and enable mathematical operations.<br>\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2) <br>\n",
    "age: Cast String to Integer<br>\n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>\n",
    "shipment_weight_kg: Cast to Double<br>\n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>\n",
    "is_expedited: Cast to Boolean<br>\n",
    "5. Naming Standardization <br>\n",
    "Source File: DF of merged(logistics_source1 & logistics_source2)<br>\n",
    "Rename: first_name to staff_first_name<br>\n",
    "Rename: last_name to staff_last_name<br>\n",
    "Rename: hub_location to origin_hub_city<br>\n",
    "6. Reordering columns logically in a better standard format:<br>\n",
    "Source File: DF of Data from all 3 files<br>\n",
    "shipment_id (Identifier), staff_first_name (Dimension)staff_last_name (Dimension), role (Dimension), origin_hub_city (Location), shipment_cost (Metric), ingestion_timestamp (Audit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "332cae58-812e-472a-bba3-b998361a792c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1.Add a column\n",
    "#: domain as 'Logistics', current timestamp 'ingestion_timestamp' and 'False' as #'is_expedited'\n",
    "addcolshipdf = shipmentsrcdf.withColumns({\"Domain\":lit(\"Logistics\"),\"ingestion_timestamp\":current_timestamp(),\"is_expedited\":lit(\"False\")})\n",
    "#display(addcolshipdf)\n",
    "\n",
    "#2.Column Uniformity: role - Convert to lowercase\n",
    "#Source File: DF of merged(logistics_source1 & logistics_source2)\n",
    "#vehicle_type - Convert values to UPPERCASE\n",
    "#Source Files: DF of logistics_shipment_detail_3000.json hub_location - Convert #values to initcap case\n",
    "colunisrc12df = scrubsrc12df.withColumns({\"role\":lower(\"role\"),\"vehicle_type\":upper(\"vehicle_type\"),\"hub_location\":initcap(\"hub_location\")})\n",
    "#display(colunisrc12df)\n",
    "\n",
    "#3.Format Standardization:\n",
    "#Source Files: DF of logistics_shipment_detail_3000.json Convert shipment_date to yyyy-MM-dd\n",
    "#Ensure shipment_cost has 2 decimal precision\n",
    "shipsrcstddf = addcolshipdf.withColumn(\"shipment_date\", to_date(col(\"shipment_date\"),'yy-MM-dd')).withColumn(\"shipment_cost\", round(\"shipment_cost\", 2))\n",
    "#display(shipsrcstddf)\n",
    "\n",
    "#4.Data Type Standardization\n",
    "#Standardizing column data types to fix schema drift and enable mathematical operations.\n",
    "#Source File:merged(logistics_source1 & logistics_source2)age: Cast String to Integer\n",
    "#Source File:logistics_shipment_detail_3000.json,shipment_weight_kg: Cast to Double\n",
    "#Source File:logistics_shipment_detail_3000.json,is_expedited: Cast to Boolean\n",
    "dtstdsrc12df = colunisrc12df.withColumn(\"age\",col(\"age\").cast(\"int\"))\n",
    "display(dtstdsrc12df)\n",
    "dtstdsrc12df.printSchema()\n",
    "dtstdshipdf = shipsrcstddf.withColumn(\"shipment_weight_kg\",col(\"shipment_weight_kg\").cast(\"double\")).withColumn(\"is_expedited\",col(\"is_expedited\").cast(\"boolean\"))\n",
    "display(dtstdshipdf)\n",
    "dtstdshipdf.printSchema()\n",
    "\n",
    "#5.Naming Standardization\n",
    "#Source File: DF of merged(logistics_source1 & logistics_source2)\n",
    "#Rename: first_name to staff_first_name\n",
    "#Rename: last_name to staff_last_name\n",
    "#Rename: hub_location to origin_hub_city\n",
    "namestdsrc12df = dtstdsrc12df.withColumnsRenamed({\"first_name\":\"staff_first_name\",\"last_name\":\"staff_last_name\",\"hub_location\":\"origin_hub_city\"})\n",
    "display(namestdsrc12df)\n",
    "\n",
    "#6.Reordering columns logically in a better standard format:\n",
    "#Source File: DF of Data from all 3 files\n",
    "#shipment_id (Identifier), staff_first_name (Dimension),staff_last_name (Dimension), role (Dimension), origin_hub_city (Location), shipment_cost (Metric), ingestion_timestamp (Audit)\n",
    "allsrcmergedf = namestdsrc12df.join(dtstdshipdf,how=\"inner\",on=\"shipment_id\").select(\"shipment_id\",\"staff_first_name\",\"staff_last_name\",\"role\",\"origin_hub_city\",\"shipment_cost\",\"ingestion_timestamp\")\n",
    "display(allsrcmergedf)\n",
    "print(\"today records after src merge\")\n",
    "print(len(allsrcmergedf.collect()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5bafa5c8-7355-4d4f-9caf-40e0e0d303f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Deduplication:\n",
    "1. Apply Record Level De-Duplication\n",
    "2. Apply Column Level De-Duplication (Primary Key Enforcement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6812c6bb-1e56-4d83-b259-1d8439dee849",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Apply Record Level De-Duplication\n",
    "dedupallsrcdf = allsrcmergedf.distinct()\n",
    "#display(dedupallsrcdf)\n",
    "print(len(dedupallsrcdf.collect()))\n",
    "\n",
    "#Apply Column Level De-Duplication (Primary Key Enforcement)\n",
    "dedupallsrcdf1 = dedupallsrcdf.dropDuplicates([\"shipment_id\"])\n",
    "#display(dedupallsrcdf1)\n",
    "print(len(dedupallsrcdf1.collect()))\n",
    "\n",
    "mungeddf = dedupallsrcdf1\n",
    "display(mungeddf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "184ecd12-8081-4c8d-925e-d47c84e520d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2. Data Enrichment - Detailing of data\n",
    "Makes your data rich and detailed <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "311d404f-b7d4-4e56-9f09-9367ec05e283",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Adding of Columns (Data Enrichment)\n",
    "*Creating new derived attributes to enhance traceability and analytical capability.*\n",
    "\n",
    "**1. Add Audit Timestamp (`load_dt`)**\n",
    "Source File: DF of logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** We need to track exactly when this record was ingested into our Data Lakehouse for auditing purposes.\n",
    "* **Action:** Add a column `load_dt` using the function `current_timestamp()`.\n",
    "\n",
    "**2. Create Full Name (`full_name`)**\n",
    "Source File: DF of logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** The reporting dashboard requires a single field for the driver's name instead of separate columns.\n",
    "* **Action:** Create `full_name` by concatenating `first_name` and `last_name` with a space separator.\n",
    "* **Result:** \"Rajesh\" + \" \" + \"Kumar\" -> **\"Rajesh Kumar\"**\n",
    "\n",
    "**3. Define Route Segment (`route_segment`)**\n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>\n",
    "* **Scenario:** The logistics team wants to analyze performance based on specific transport lanes (Source to Destination).\n",
    "* **Action:** Combine `source_city` and `destination_city` with a hyphen.\n",
    "* **Result:** \"Chennai\" + \"-\" + \"Pune\" -> **\"Chennai-Pune\"**\n",
    "\n",
    "**4. Generate Vehicle Identifier (`vehicle_identifier`)**\n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>\n",
    "* **Scenario:** We need a unique tracking code that immediately tells us the vehicle type and the shipment ID.\n",
    "* **Action:** Combine `vehicle_type` and `shipment_id` to create a composite key.\n",
    "* **Result:** \"Truck\" + \"_\" + \"500001\" -> **\"Truck_500001\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2313b20e-968d-4bee-8e3a-f1d2c4de6a5c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768588060801}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Add Audit Timestamp \n",
    "#Action: Add a column load_dt using the function current_timestamp()\n",
    "namestd1src12df = namestdsrc12df.distinct()\n",
    "namestd2src12df = namestd1src12df.dropDuplicates([\"shipment_id\"])\n",
    "\n",
    "denrichsrc12df = namestdsrc12df.withColumn(\"load_dt\",current_timestamp())\n",
    "#display(denrichsrc12df)\n",
    "#2. Create Full Name (full_name) \n",
    "#Action: Create full_name by concatenating first_name and last_name with a space separator.\n",
    "denrichsrc12df1 = denrichsrc12df.select(\"shipment_id\",\n",
    "concat_ws(\" \",col(\"staff_first_name\"),col(\"staff_last_name\")).alias(\"full_name\"),\"staff_first_name\",\"staff_last_name\",\"age\",\"role\",\"data_source\",\"origin_hub_city\",\"vehicle_type\",\"load_dt\")\n",
    "#display(denrichsrc12df1)\n",
    "\n",
    "#3.Define Route Segment (route_segment)\n",
    "#Action: Combine source_city and destination_city with a hyphen.\n",
    "display(dtstdshipdf.take(5))\n",
    "denrichshipdf = dtstdshipdf.select(\"shipment_id\",\"order_id\",concat(\"source_city\",lit('-'),\"destination_city\").alias(\"route_segment\"),\"shipment_status\",\"cargo_type\",\"vehicle_type\",\"payment_mode\",\"shipment_weight_kg\",\"shipment_cost\",\"shipment_date\")\n",
    "#display(denrichshipdf)\n",
    "\n",
    "#4.Generate Vehicle Identifier (vehicle_identifier) \n",
    "#Action: Combine vehicle_type and shipment_id to create a composite key\n",
    "denrichshipdf1 = denrichshipdf.select(\"shipment_id\",concat(\"vehicle_type\",lit('_'),\"shipment_id\").alias(\"vehicle_identifier\"),\"order_id\",\"route_segment\",\"shipment_status\",\"cargo_type\",\"payment_mode\",\"shipment_weight_kg\",\"shipment_cost\",\"shipment_date\")\n",
    "display(denrichshipdf1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e525645-18d1-4a16-9909-afc89a2ed57d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Deriving of Columns (Time Intelligence)\n",
    "*Extracting temporal features from dates to enable period-based analysis and reporting.*<br>\n",
    "Source File: logistics_shipment_detail_3000.json<br>\n",
    "**1. Derive Shipment Year (`shipment_year`)**\n",
    "* **Scenario:** Management needs an annual performance report to compare growth year-over-year.\n",
    "* **Action:** Extract the year component from `shipment_date`.\n",
    "* **Result:** \"2024-04-23\" -> **2024**\n",
    "\n",
    "**2. Derive Shipment Month (`shipment_month`)**\n",
    "* **Scenario:** Analysts want to identify seasonal peaks (e.g., increased volume in December).\n",
    "* **Action:** Extract the month component from `shipment_date`.\n",
    "* **Result:** \"2024-04-23\" -> **4** (April)\n",
    "\n",
    "**3. Flag Weekend Operations (`is_weekend`)**\n",
    "* **Scenario:** The Operations team needs to track shipments handled during weekends to calculate overtime pay or analyze non-business day capacity.\n",
    "* **Action:** Flag as **'True'** if the `shipment_date` falls on a Saturday or Sunday.\n",
    "\n",
    "**4. Flag shipment status (`is_expedited`)**\n",
    "* **Scenario:** The Operations team needs to track shipments is IN_TRANSIT or DELIVERED.\n",
    "* **Action:** Flag as **'True'** if the `shipment_status` IN_TRANSIT or DELIVERED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8dea580-0317-4041-b4cd-fd45151b9276",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768767233148}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1.Derive Shipment Year (shipment_year), 2. Shipment Month (shipment_month)\n",
    "display(denrichshipdf1)\n",
    "coldershipdf = denrichshipdf1.withColumns({\"shipment_year\":year(col(\"shipment_date\")),\"shipment_month\":month(col(\"shipment_date\"))})\n",
    "\n",
    "#3.Flag Weekend Operations (is_weekend)\n",
    "coldershipdf1 = coldershipdf.withColumn(\"is_weekend\",when(dayofweek(col(\"shipment_date\")).isin([1,7]),True).otherwise(False))\n",
    "\n",
    "#4.Flag shipment status (is_expedited)\n",
    "coldershipdf2 = coldershipdf1.withColumn(\"is_expedited\",when(col(\"shipment_status\").isin([\"IN_TRANSIT\",\"DELIVERED\"]),True))\n",
    "display(coldershipdf2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f178441f-3675-448e-b8f1-f45336851f82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Enrichment/Business Logics (Calculated Fields)\n",
    "*Deriving new metrics and financial indicators using mathematical and date-based operations.*<br>\n",
    "Source File: logistics_shipment_detail_3000.json<br>\n",
    "\n",
    "**1. Calculate Unit Cost (`cost_per_kg`)**\n",
    "* **Scenario:** The Finance team wants to analyze the efficiency of shipments by determining the cost incurred per unit of weight.\n",
    "* **Action:** Divide `shipment_cost` by `shipment_weight_kg`.\n",
    "* **Logic:** `shipment_cost / shipment_weight_kg`\n",
    "\n",
    "**2. Track Shipment Age (`days_since_shipment`)**\n",
    "* **Scenario:** The Operations team needs to monitor how long it has been since a shipment was dispatched to identify potential delays.\n",
    "* **Action:** Calculate the difference in days between the `current_date` and the `shipment_date`.\n",
    "* **Logic:** `datediff(current_date(), shipment_date)`\n",
    "\n",
    "**3. Compute Tax Liability (`tax_amount`)**\n",
    "* **Scenario:** For invoicing and compliance, we must calculate the Goods and Services Tax (GST) applicable to each shipment.\n",
    "* **Action:** Calculate 18% GST on the total `shipment_cost`.\n",
    "* **Logic:** `shipment_cost * 0.18`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2308f2b5-b873-4639-8c51-30a8c28bc85e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import col, lit, datediff, current_date\n",
    "#1.Calculate Unit Cost (cost_per_kg): Action: Divide shipment_cost by shipment_weight_kg.\n",
    "#2.Track Shipment Age (days_since_shipment): Calculate the difference in days between the current_date and the shipment_date.\n",
    "#3. Compute Tax Liability (tax_amount): Calculate 18% GST on the total shipment_cost.\n",
    "blshipdf = coldershipdf2\\\n",
    "    .withColumn(\"cost_per_kg\", round(col(\"shipment_cost\")/col(\"shipment_weight_kg\"),2))\\\n",
    "    .withColumn(\"days_since_shipment\", datediff(current_date(), col(\"shipment_date\")))\\\n",
    "    .withColumn(\"tax_amount\", round(col(\"shipment_cost\")*lit(0.18),2))\n",
    "display(blshipdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9de85d4f-d903-46fd-b110-1476a2383d6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Remove/Eliminate (drop, select, selectExpr)\n",
    "*Excluding unnecessary or redundant columns to optimize storage and privacy.*<br>\n",
    "Source File: DF of logistics_source1 and logistics_source2<br>\n",
    "\n",
    "**1. Remove Redundant Name Columns**\n",
    "* **Scenario:** Since we have already created the `full_name` column in the Enrichment step, the individual name columns are now redundant and clutter the dataset.\n",
    "* **Action:** Drop the `first_name` and `last_name` columns.\n",
    "* **Logic:** `df.drop(\"first_name\", \"last_name\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e32c48b0-076b-4537-9a47-33ede124d6a7",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768767554924}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(denrichsrc12df1)\n",
    "cleansedsrc12df = denrichsrc12df1.drop(\"staff_first_name\",\"staff_last_name\")\n",
    "display(cleansedsrc12df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7682d4f7-a188-4f86-b60f-c1afa5db220f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Splitting & Merging/Melting of Columns\n",
    "*Reshaping columns to extract hidden values or combine fields for better analysis.*<br>\n",
    "Source File: DF of logistics_shipment_detail_3000.json<br>\n",
    "**1. Splitting (Extraction)**\n",
    "*Breaking one column into multiple to isolate key information.*\n",
    "* **Split Order Code:**\n",
    "  * **Action:** Split `order_id` (\"ORD100000\") into two new columns:\n",
    "    * `order_prefix` (\"ORD\")\n",
    "    * `order_sequence` (\"100000\")\n",
    "* **Split Date:**\n",
    "  * **Action:** Split `shipment_date` into three separate columns for partitioning:\n",
    "    * `ship_year` (2024)\n",
    "    * `ship_month` (4)\n",
    "    * `ship_day` (23)\n",
    "\n",
    "**2. Merging (Concatenation)**\n",
    "*Combining multiple columns into a single unique identifier or description.*\n",
    "* **Create Route ID:**\n",
    "  * **Action:** Merge `source_city` (\"Chennai\") and `destination_city` (\"Pune\") to create a descriptive route key:\n",
    "    * `route_lane` (\"Chennai->Pune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0d4c66d-0758-4d08-85ae-a054db73255a",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768601746204}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Splitting and Merging shipment columns"
    }
   },
   "outputs": [],
   "source": [
    "#1.Splitting (Extraction): \n",
    "# Split order_id (\"ORD100000\") into:order_prefix (\"ORD\") & #order_sequence (\"100000\")\n",
    "#Split shipment_date into:ship_year (2024),ship_month (4) & ship_day (23)\n",
    "#display(blshipdf) #shipment year & month are already classified, only day is derived here and renaming year and month\n",
    "splitshipdf = blshipdf.withColumn(\"order_prefix\",substring(col(\"order_id\"),1,3))\\\n",
    "    .withColumn(\"order_sequence\",substring(col(\"order_id\"),4,9))\\\n",
    "    .withColumn(\"ship_day\",day(col(\"shipment_date\")))\\\n",
    "    .withColumnsRenamed({\"shipment_year\":\"ship_year\",\"shipment_month\":\"ship_month\"})\n",
    "display(splitshipdf)\n",
    "\n",
    "#2.Merging (Concatenation): Create Route ID: Use route_segment to create route_lane\n",
    "#already we have route_segment showing same data, we can rename the column and enrich more\n",
    "mergedshipdf = splitshipdf.withColumnRenamed(\"route_segment\",\"route_lane\")\n",
    "display(mergedshipdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8919a94-2066-4b45-ba07-e30f05744c81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3. Data Customization & Processing - Application of Tailored Business Specific Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3a4891e-0c24-40b4-8ed8-b124efed02f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **UDF1: Complex Incentive Calculation**\n",
    "**Scenario:** The Logistics Head wants to calculate a \"Performance Bonus\" for drivers based on tenure and role complexity.\n",
    "\n",
    "**Action:** Create a Python function `calculate_bonus(role, age)` and register it as a Spark UDF.\n",
    "\n",
    "**Logic:**\n",
    "* **IF** `Role` == 'Driver' **AND** `Age` > 50:\n",
    "  * `Bonus` = 15% of Salary (Reward for Seniority)\n",
    "* **IF** `Role` == 'Driver' **AND** `Age` < 30:\n",
    "  * `Bonus` = 5% of Salary (Encouragement for Juniors)\n",
    "* **ELSE**:\n",
    "  * `Bonus` = 0\n",
    "\n",
    "**Result:** A new derived column `projected_bonus` is generated for every row in the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### **UDF2: PII Masking (Privacy Compliance)**\n",
    "**Scenario:** For the analytics dashboard, we must hide the full identity of the staff to comply with privacy laws (GDPR/DPDP), while keeping names recognizable for internal managers.\n",
    "\n",
    "**Business Rule:** Show the first 2 letters, mask the middle characters with `****`, and show the last letter.\n",
    "\n",
    "**Action:** Create a UDF `mask_identity(name)`.\n",
    "\n",
    "**Example:**\n",
    "* **Input:** `\"Rajesh\"`\n",
    "* **Output:** `\"Ra****h\"`\n",
    "<br>\n",
    "**Note: Convert the above udf logic to inbult function based transformation to ensure the performance is improved.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dea166e9-ba51-4ed6-9958-dd55b9d7a925",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Create a Python function calculate_bonus(role, age) and register it as a Spark UDF.\n",
    "def calculate_bonus(role, age):\n",
    "\tif role == \"driver\" and age > 50:\n",
    "\t    return  0.15\n",
    "\telif role == \"driver\" and age < 30:\n",
    "\t    return 0.05\n",
    "\telse:\n",
    "\t    return 0\n",
    "\n",
    "#Create a UDF mask_identity(name).\n",
    "def mask_identity(name):\n",
    "    return name[0:2] + \"*\" * (len(name) - 3) + name[-1]\n",
    "\n",
    "#A new derived column projected_bonus is generated for every row in the dataset &  Convert the above udf logic to inbult function based transformation to ensure the performance is improved.\n",
    "bonuspct_udf = udf(calculate_bonus)\n",
    "namemask_udf = udf(mask_identity)\n",
    "\n",
    "dcussrc12df = cleansedsrc12df.withColumn(\"projected_bonus\",bonuspct_udf(col(\"role\"),col(\"age\")))\\\n",
    ".withColumn(\"masked_name\",namemask_udf(col(\"full_name\")))\n",
    "display(dcussrc12df)\n",
    "#display(cleansedsrc12df.filter(col(\"role\") ==\"driver\"))\n",
    "\n",
    "dcussrc12df1 = cleansedsrc12df.withColumn(\"projected_bonus\",\n",
    "when((col(\"role\") == \"driver\") & (col(\"age\") >50), 0.15)\n",
    ".when((col(\"role\") == \"driver\") & (col(\"age\") <30), 0.05)\n",
    ".otherwise(0))\\\n",
    ".withColumn(\"masked_name\",when((length(col(\"full_name\"))>2),\n",
    "concat(substring(col(\"full_name\"),1,2),\n",
    "repeat(\"*\",length(col(\"full_name\"))-3),\n",
    "substring(col(\"full_name\"),-1,1))))\n",
    "display(dcussrc12df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ed3d160-f54e-4bf3-bb2c-40ae7ff1b7d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Data Core Curation & Processing (Pre-Wrangling)\n",
    "*Applying business logic to focus, filter, and summarize data before final analysis.*\n",
    "\n",
    "**1. Select (Projection)**<br>\n",
    "Source Files: DF of logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** The Driver App team only needs location data, not sensitive HR info.\n",
    "* **Action:** Select only `first_name`, `role`, and `hub_location`.\n",
    "\n",
    "**2. Filter (Selection)**<br>\n",
    "Source File: DF of json<br>\n",
    "* **Scenario:** We need a report on active operational problems.\n",
    "* **Action:** Filter rows where `shipment_status` is **'DELAYED'** or **'RETURNED'**.\n",
    "* **Scenario:** Insurance audit for senior staff.\n",
    "* **Action:** Filter rows where `age > 50`.\n",
    "\n",
    "**3. Derive Flags & Columns (Business Logic)**<br>\n",
    "Source File: DF of json<br>\n",
    "* **Scenario:** Identify high-value shipments for security tracking.\n",
    "* **Action:** Create flag `is_high_value` = **True** if `shipment_cost > 50,000`.\n",
    "* **Scenario:** Flag weekend operations for overtime calculation.\n",
    "* **Action:** Create flag `is_weekend` = **True** if day is Saturday or Sunday.\n",
    "\n",
    "**4. Format (Standardization)**<br>\n",
    "Source File: DF of json<br>\n",
    "* **Scenario:** Finance requires readable currency formats.\n",
    "* **Action:** Format `shipment_cost` to string like **\"₹30,695.80\"**.\n",
    "* **Scenario:** Standardize city names for reporting.\n",
    "* **Action:** Format `source_city` to Uppercase (e.g., \"chennai\" → **\"CHENNAI\"**).\n",
    "\n",
    "**5. Group & Aggregate (Summarization)**<br>\n",
    "Source Files: DF of logistics_source1 and logistics_source2<br>\n",
    "* **Scenario:** Regional staffing analysis.\n",
    "* **Action:** Group by `hub_location` and **Count** the number of staff.\n",
    "* **Scenario:** Fleet capacity analysis.\n",
    "* **Action:** Group by `vehicle_type` and **Sum** the `shipment_weight_kg`.\n",
    "\n",
    "**6. Sorting (Ordering)**<br>\n",
    "Source File: DF of json<br>\n",
    "* **Scenario:** Prioritize the most expensive shipments.\n",
    "* **Action:** Sort by `shipment_cost` in **Descending** order.\n",
    "* **Scenario:** Organize daily dispatch schedule.\n",
    "* **Action:** Sort by `shipment_date` (Ascending).\n",
    "\n",
    "**7. Limit (Top-N Analysis)**<br>\n",
    "Source File: DF of json<br>\n",
    "* **Scenario:** Dashboard snapshot of critical delays.\n",
    "* **Action:** Filter for 'DELAYED', Sort by Cost, and **Limit to top 10** rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b01fe19d-0418-4f35-8255-22ffaf5fd601",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768713256563}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Data core curation"
    }
   },
   "outputs": [],
   "source": [
    "#Business logics & transformations\n",
    "\n",
    "#if the business logic to be derived here requires \"when condition\", then its better to use \"selectExpr()\" as it is mere english kinda code(SQL). refer vvimp. notebook for reference.\n",
    "\n",
    "#display(mergedshipdf)\n",
    "#display(cleansedsrc12df)\n",
    "#1.Select (Projection): src12 - Select only first_name, role, and hub_location\n",
    "dccursrc12df = cleansedsrc12df.select(\"full_name\",\"role\",\"origin_hub_city\")\n",
    "display(dccursrc12df)\n",
    "\n",
    "#2. Filter (Selection): shipsrc - \n",
    "#Action: Filter rows where shipment_status is 'DELAYED' or 'RETURNED'.\n",
    "#Scenario: Insurance audit for senior staff: src12 -\n",
    "#Action: Filter rows where age > 50.\n",
    "dccurshipdf = mergedshipdf.filter(col(\"shipment_status\").isin(\"DELAYED\",\"RETURNED\"))\n",
    "dccursrc12df1 = cleansedsrc12df.select(\"*\").filter(col(\"age\") > 50)\n",
    "#display(dccurshipdf)\n",
    "#display(dccursrc12df1)\n",
    "\n",
    "#3.Derive Flags & Columns (Business Logic): shipsrc - \n",
    "#Scenario: Identify high-value shipments for security tracking.\n",
    "#Action: Create flag is_high_value = True if shipment_cost > 50,000.\n",
    "#Scenario: Flag weekend operations for overtime calculation. - ignore already done\n",
    "#****Action: Create flag is_weekend = True if day is Saturday or Sunday.\n",
    "\n",
    "#display(mergedshipdf.filter(col(\"shipment_Cost\") > 50000)) # no records found\n",
    "dccurshipdf1 = mergedshipdf.withColumn(\"is_high_value\", when(col(\"shipment_cost\") > 50000, True))\n",
    "#display(dccurshipdf1)\n",
    "\n",
    "#4.Format (Standardization): shipsrc -\n",
    "#Scenario: Finance requires readable currency formats.\n",
    "#Action: Format shipment_cost to string like \"₹30,695.80\".\n",
    "#Scenario: Standardize city names for reporting.- since we dropped source & dest. city columns, we can upper route_lane.\n",
    "#Action: Format source_city to Uppercase (e.g., \"chennai\" → \"CHENNAI\")\n",
    "dccurshipdf2 = dccurshipdf1.selectExpr(\n",
    "    \"shipment_id\",\n",
    "    \"vehicle_identifier\",\n",
    "    \"order_id\",\n",
    "    \"route_lane\",\n",
    "    \"shipment_status\",\n",
    "    \"cargo_type\",\n",
    "    \"payment_mode\",\n",
    "    \"shipment_weight_kg\",\n",
    "    \"concat('₹',shipment_cost) as shipment_cost\",\n",
    "    \"shipment_date\",\n",
    "    \"ship_year\",\n",
    "    \"ship_month\",\n",
    "    \"ship_day\",\n",
    "    \"is_weekend\",\n",
    "    \"is_expedited\",\n",
    "    \"cost_per_kg\",\n",
    "    \"days_since_shipment\",\n",
    "    \"tax_amount\",\n",
    "    \"order_prefix\",\n",
    "    \"order_sequence\"\n",
    ").withColumn(\"route_lane\", upper(col(\"route_lane\")))\n",
    "display(dccurshipdf2)\n",
    "                                        \n",
    "#5.Group & Aggregate (Summarization): src12 -\n",
    "#Scenario: Regional staffing analysis.\n",
    "#Action: Group by hub_location and Count the number of staff.\n",
    "#Scenario: Fleet capacity analysis.: shipsrc - \n",
    "#Action: Group by vehicle_type and Sum the shipment_weight_kg\n",
    "display(cleansedsrc12df)\n",
    "dccursrc12df2 = cleansedsrc12df.groupBy(\"origin_hub_city\").agg(count(\"full_name\").alias(\"staff_count\"))\n",
    "dccurshipdf3 = dccurshipdf2.groupBy(split(col(\"vehicle_identifier\"),\"_\")[0]).agg(sum(\"shipment_weight_kg\").alias(\"fleet_capacity\"))\n",
    "#display(dccursrc12df2)\n",
    "#display(dccurshipdf3)\n",
    "\n",
    "#6.Sorting (Ordering): shipsrc -\n",
    "#Scenario: Prioritize the most expensive shipments.\n",
    "#Action: Sort by shipment_cost in Descending order.\n",
    "#Scenario: Organize daily dispatch schedule.\n",
    "#Action: Sort by shipment_date (Ascending).\n",
    "dccurshipdf4 = dccurshipdf2.orderBy(col(\"shipment_cost\").desc())\n",
    "dccurshipdf5 = dccurshipdf2.orderBy(col(\"shipment_date\").asc())\n",
    "#display(dccurshipdf4.take(10))\n",
    "#display(dccurshipdf5.take(10))\n",
    "\n",
    "#7.Limit (Top-N Analysis):shipsrc -Filter for 'DELAYED', Sort by Cost, and Limit to top 10 rows\n",
    "dccurshipdf6 = dccurshipdf2.filter(col(\"shipment_status\") == \"DELAYED\")\\\n",
    ".orderBy(col(\"shipment_Cost\").desc())\\\n",
    ".limit(10)\n",
    "#display(dccurshipdf6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ace35ea6-346a-4f54-97ac-dba0be0c4ca8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Data Wrangling - Transformation & Analytics\n",
    "*Combining, modeling, and analyzing data to answer complex business questions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9992924-2d11-4cfa-b8fa-5c96bf6d0475",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "### **1. Joins**\n",
    "Source Files:<br>\n",
    "Left Side (staff_df):<br> DF of logistics_source1 & logistics_source2<br>\n",
    "Right Side (shipments_df):<br> DF of logistics_shipment_detail_3000.json<br>\n",
    "#### **1.1 Frequently Used Simple Joins (Inner, Left)**\n",
    "* **Inner Join (Performance Analysis):**\n",
    "  * **Scenario:** We only want to analyze *completed work*. Connect Staff to the Shipments they handled.\n",
    "  * **Action:** Join `staff_df` and `shipments_df` on `shipment_id`.\n",
    "  * **Result:** Returns only rows where a staff member is assigned to a valid shipment.\n",
    "* **Left Join (Idle Resource check):**\n",
    "  * **Scenario:** Find out which staff members are currently *idle* (not assigned to any shipment).\n",
    "  * **Action:** Join `staff_df` (Left) with `shipments_df` (Right) on `shipment_id`. Filter where `shipments_df.shipment_id` is NULL.\n",
    "\n",
    "#### **1.2 Infrequent Simple Joins (Self, Right, Full, Cartesian)**\n",
    "* **Self Join (Peer Finding):**\n",
    "  * **Scenario:** Find all pairs of employees working in the same `hub_location`.\n",
    "  * **Action:** Join `staff_df` to itself on `hub_location`, filtering where `staff_id_A != staff_id_B`.\n",
    "* **Right Join (Orphan Data Check):**\n",
    "  * **Scenario:** Identify shipments in the system that have *no valid driver* assigned (Data Integrity Issue).\n",
    "  * **Action:** Join `staff_df` (Left) with `shipments_df` (Right). Focus on NULLs on the left side.\n",
    "* **Full Outer Join (Reconciliation):**\n",
    "  * **Scenario:** A complete audit to find *both* idle drivers AND unassigned shipments in one view.\n",
    "  * **Action:** Perform a Full Outer Join on `shipment_id`.\n",
    "* **Cartesian/Cross Join (Capacity Planning):**\n",
    "  * **Scenario:** Generate a schedule of *every possible* driver assignment to *every* pending shipment to run an optimization algorithm.\n",
    "  * **Action:** Cross Join `drivers_df` and `pending_shipments_df`.\n",
    "\n",
    "#### **1.3 Advanced Joins (Semi and Anti)**\n",
    "* **Left Semi Join (Existence Check):**\n",
    "  * **Scenario:** \"Show me the details of Drivers who have *at least one* shipment.\" (Standard filtering).\n",
    "  * **Action:** `staff_df.join(shipments_df, \"shipment_id\", \"left_semi\")`.\n",
    "  * **Benefit:** Performance optimization; it stops scanning the right table once a match is found.\n",
    "* **Left Anti Join (Negation Check):**\n",
    "  * **Scenario:** \"Show me the details of Drivers who have *never* touched a shipment.\"\n",
    "  * **Action:** `staff_df.join(shipments_df, \"shipment_id\", \"left_anti\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5238eb77-34d7-473b-b9f9-8652eba9d256",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768840241739}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Joins\n",
    "lsrcdf = cleansedsrc12df\n",
    "rshipdf = dccurshipdf2\n",
    "display(lsrcdf)\n",
    "display(rshipdf)\n",
    "\n",
    "#1.1.Frequently Used Simple Joins (Inner, Left)\n",
    "#Inner Join (Performance Analysis):Join staff_df and shipments_df on shipment_id.\n",
    "innerjoindf = lsrcdf.join(rshipdf,how=\"inner\",on=\"shipment_id\")\n",
    "print(\"inner join\")\n",
    "#display(innerjoindf)\n",
    "\n",
    "#Left Join (Idle Resource check):Join staff_df (Left) with shipments_df (Right) on shipment_id. Filter where shipments_df.shipment_id is NULL.\n",
    "leftjoindf = lsrcdf.join(rshipdf,how=\"left\",on=\"shipment_id\").filter(col(\"shipment_id\").isNull())\n",
    "print(\"left join\")\n",
    "#display(leftjoindf)\n",
    "\n",
    "#1.2.Infrequent Simple Joins (Self, Right, Full, Cartesian)\n",
    "#Self Join (Peer Finding):Join staff_df to itself on hub_location, filtering where staff_id_A != staff_id_B.\n",
    "#*******ERROR FIXED\n",
    "selfjoindf = lsrcdf.alias(\"l\").join(lsrcdf.alias(\"r\"),how=\"inner\",on=\"origin_hub_city\").filter(col(\"l.shipment_id\") != col(\"r.shipment_id\"))\n",
    "print(\"self join\")\n",
    "selfjoindf.select(\"l.shipment_id\",\"r.*\").display(10)\n",
    "\n",
    "#Right Join (Orphan Data Check):Join staff_df (Left) with shipments_df (Right). Focus on NULLs on the left side.\n",
    "rightjoindf = lsrcdf.alias(\"l\").join(rshipdf.alias(\"r\"),how=\"right\",on=\"shipment_id\").filter(col(\"l.shipment_id\").isNull())\n",
    "#display(rightjoindf)\n",
    "\n",
    "#Full Outer Join (Reconciliation):Perform a Full Outer Join on shipment_id.\n",
    "fulljoindf = lsrcdf.join(rshipdf,how=\"full\",on=\"shipment_id\")\n",
    "print(\"full join\")\n",
    "#display(fulljoindf)\n",
    "\n",
    "#Cartesian/Cross Join (Capacity Planning):Cross Join drivers_df and pending_shipments_df.\n",
    "l1srcdf = lsrcdf.filter(col(\"role\") == \"driver\")\n",
    "r1shipdf = dccurshipdf6\n",
    "cartjoindf = l1srcdf.join(rshipdf)\n",
    "print(\"catesian join\")\n",
    "#display(cartjoindf)\n",
    "\n",
    "#1.3.Advanced Joins (Semi and Anti)\n",
    "#Left Semi Join (Existence Check):staff_df.join(shipments_df, \"shipment_id\", \"left_semi\")\n",
    "lsemijoindf = lsrcdf.join(rshipdf,how=\"left_semi\",on=\"shipment_id\")\n",
    "print(\"semi join\")\n",
    "#display(lsemijoindf)\n",
    "\n",
    "#Left Anti Join (Negation Check):staff_df.join(shipments_df, \"shipment_id\", \"left_anti\")\n",
    "lantijoindf = lsrcdf.join(rshipdf,how=\"left_anti\",on=\"shipment_id\")\n",
    "print(\"anti join\")\n",
    "#display(lantijoindf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bc01963-7886-4248-bdb8-202e038b1a85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **2. Lookup**<br>\n",
    "Source File: DF of logistics_source1 and logistics_source2 (merged into Staff DF) and Master_City_List.csv dataframe<br>\n",
    "* **Scenario:** Validation. Check if the `hub_location` in the staff file exists in the in the dataframe of corporate `Master_City_List`.\n",
    "* **Action:** Compare values against this Master_City_List list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57bb29d8-24cb-4130-8b95-230787b39212",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mcldf = spark.read.csv(\"/Volumes/logistics_catalog/landing_zone/landing_vol/Master_City_List.csv\",header=True,mode='permissive')\n",
    "#display(mcldf)\n",
    "\n",
    "mcldf1 = lsrcdf.join(mcldf,how=\"semi\",on=(col(\"origin_hub_city\")==col(\"city_name\")))\n",
    "#display(mcldf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70a72d3d-11e9-4da1-8c89-8fe76950276a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **3. Lookup & Enrichment**<br>\n",
    "Source File: DF of logistics_source1 and logistics_source2 (merged into Staff DF) and Master_City_List.csv dataframe<br>\n",
    "* **Scenario:** Geo-Tagging.\n",
    "* **Action:** Lookup hub_location (eg. \"Pune\") in a Master Latitude/Longitude Master_City_List.csv dataframe and enrich our logistics_source (merged dataframe) by adding lat and long columns for map plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1063f2cb-e046-46ec-a0f4-c829b49b56d3",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768840529749}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(lsrcdf)\n",
    "lsrcdf1 = lsrcdf.withColumnRenamed(\"origin_hub_city\",\"city_name\")\n",
    "luenrichdf = lsrcdf1.alias(\"l\").join(mcldf.alias(\"r\"), how='left', on=col(\"l.city_name\") == col(\"r.city_name\"))\\\n",
    "    .select(\"l.*\", \"r.latitude\", \"r.longitude\")\n",
    "display(luenrichdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a752dfca-7d7f-4232-98fb-6dd23d0ff453",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **4. Schema Modeling (Denormalization)**<br>\n",
    "Source Files: DF of All 3 Files (logistics_source1, logistics_source2, logistics_shipment_detail_3000.json)<br>\n",
    "* **Scenario:** Creating a \"Gold Layer\" Table for PowerBI/Tableau.\n",
    "* **Action:** Flatten the Star Schema. Join `Staff`, `Shipments`, and `Vehicle_Master` into one wide table (`wide_shipment_history`) so analysts don't have to perform joins during reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1d645d1-74e8-440c-a8da-bdeb99bf9353",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "wide_shipment_historydf = lsrcdf.join(rshipdf,how='inner',on=\"shipment_id\")\\\n",
    ".join(mcldf,how='inner',on=(col(\"city_name\")==col(\"origin_hub_city\")))\n",
    "display(wide_shipment_historydf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c4ef064-31aa-4188-a79a-dc47c56c28df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **5. Windowing (Ranking & Trends)**<br>\n",
    "Source Files:<br>\n",
    "DF of logistics_source2: Provides hub_location (Partition Key).<br>\n",
    "logistics_shipment_detail_3000.json: Provides shipment_cost (Ordering Key)<br>\n",
    "* **Scenario:** \"Who are the Top 3 Drivers by Cost in *each* Hub?\"\n",
    "* **Action:**\n",
    "  1. Partition by `hub_location`.\n",
    "  2. Order by `total_shipment_cost` Descending.\n",
    "  3. Apply `dense_rank()` and `row_number()\n",
    "  4. Filter where `rank or row_number <= 3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36f2154b-7d9d-4037-88db-6f8f0c58444d",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768858000919}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "#display(src2df)\n",
    "windjoindf = src2df.join(shipmentsrcdf,how='inner',on=\"shipment_id\")\n",
    "#display(windjoindf)\n",
    "\n",
    "#using row_number()\n",
    "windowdf = windjoindf.withColumn(\"seqnum\",row_number().over(Window.partitionBy(\"hub_location\").orderBy(desc(\"shipment_cost\"))))\n",
    "#display(windowdf)\n",
    "display(windowdf.where(\"seqnum <=3\"))\n",
    "\n",
    "#using dense_Rank()\n",
    "windowdf = windjoindf.withColumn(\"dnsrank\",dense_rank().over(Window.partitionBy(    \"hub_location\").orderBy(desc(\"shipment_Cost\")))).where(\"dnsrank <=3\")\n",
    "display(windowdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d399684-49c4-40e8-ad4c-8867578e6ab3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **6. Analytical Functions (Lead/Lag)**<br>\n",
    "Source File: <br>\n",
    "DF of logistics_shipment_detail_3000.json<br>\n",
    "* **Scenario:** Idle Time Analysis.\n",
    "* **Action:** For each driver, calculate the days elapsed since their *previous* shipment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d4a90ba-b01f-4062-ae1e-103b807b696e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db195cbf-324e-42b1-91e7-2916cbf33e31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **7. Set Operations**<br>\n",
    "Source Files: DF of logistics_source1 and logistics_source2<br>\n",
    "* **Union:** Combining `Source1` (Legacy) and `Source2` (Modern) into one dataset (Already done in Active Munging).\n",
    "* **Intersect:** Identifying Staff IDs that appear in *both* Source 1 and Source 2 (Duplicate/Migration Check).\n",
    "* **Except (Difference):** Identifying Staff IDs present in Source 2 but *missing* from Source 1 (New Hires).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bcf000f-c221-4a91-b3d8-621181bfadb1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set Operations with Common Columns"
    }
   },
   "outputs": [],
   "source": [
    "# Find common columns between src1df and src2df\n",
    "common_cols = [col for col in src1df.columns if col in src2df.columns]\n",
    "\n",
    "#Union\n",
    "setuniondf = src1df.select(common_cols).union(src2df.select(common_cols)).distinct()\n",
    "setuniondf.show(20)\n",
    "\n",
    "#intersect\n",
    "setintdf = src1df.select(common_cols).intersect(src2df.select(common_cols))\n",
    "setintdf.show(20)\n",
    "\n",
    "#Difference\n",
    "setdiffdf = src1df.select(common_cols).subtract(src2df.select(common_cols))\n",
    "setdiffdf.show(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb15b6e2-d66c-492a-a31f-1bb346b5c13a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **8. Grouping & Aggregations (Advanced)**<br>\n",
    "Source Files:<br>\n",
    "DF of logistics_source2: Provides hub_location and vehicle_type (Grouping Dimensions).<br>\n",
    "DF of logistics_shipment_detail_3000.json: Provides shipment_cost (Aggregation Metric).<br>\n",
    "* **Scenario:** The CFO wants a subtotal report at multiple levels:\n",
    "  1. Total Cost by Hub.\n",
    "  2. Total Cost by Hub AND Vehicle Type.\n",
    "  3. Grand Total.\n",
    "* **Action:** Use `cube(\"hub_location\", \"vehicle_type\")` or `rollup()` to generate all these subtotals in a single query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c05d6556-1971-4177-a741-3329b167ad23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#****Error\n",
    "\n",
    "windjoindf = src2df.alias(\"l\").join(shipmentsrcdf.alias(\"r\"),how='inner',on=\"shipment_id\")\n",
    "analyticdf = windowdf.rollup(\"l.hub_location\").agg(sum(\"r.shipment_cost\").alias(\"r.tot_hub_cost\")).orderBy(\"l.hub_location\")\n",
    "analyticdf = windowdf.cube(\"l.hub_location\",\"l.vehicle_type\").agg(sum(\"r.shipment_cost\").alias(\"r.tot_cost_hub_vehtype\")).orderBy(\"l.hub_location\",\"r.tot_hub_cost\")\n",
    "display(analyticdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fbdbf31-7a73-44f2-8e74-6c6fcf35d916",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##6. Data Persistance (LOAD)-> Data Publishing & Consumption<br>\n",
    "\n",
    "Store the inner joined, lookup and enrichment, Schema Modeling, windowing, analytical functions, set operations, grouping and aggregation data into the delta tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aee7e8d0-1fbb-4a6c-af39-316374a94a67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##7.Take the copy of the above notebook and try to write the equivalent SQL for which ever applicable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39d4fbd8-ef47-4480-996b-715252570127",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####SQL is best to use for \n",
    "1. when conditions\n",
    "2. groupby & aggregation , orderby & limit\n",
    "3. re-ordering & re-formatting of columns\n",
    "4. windowing operations\n",
    "5. no unionByName avail in SQL\n",
    "6. uses minus (no subtract avail)\n",
    "\n",
    "####DSL is best to use for\n",
    "1. ingress of file & egress of file or table\n",
    "2. operations where functions are critical to use(like na.drop/fill/replace etc)\n",
    "3. pivoting in dsl is better \n",
    "4. unionByNAme is useful here for dat munching\n",
    "5. uses subtract (no minus avail)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BB2_Usecase2_DSL_SQL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
